{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7307833\n",
      "200 0.5715119\n",
      "400 0.5074139\n",
      "600 0.4718242\n",
      "800 0.44758478\n",
      "1000 0.42857108\n",
      "1200 0.41232458\n",
      "1400 0.39775506\n",
      "1600 0.38433787\n",
      "1800 0.37180114\n",
      "2000 0.35999325\n",
      "2200 0.34882212\n",
      "2400 0.3382263\n",
      "2600 0.32816055\n",
      "2800 0.31858906\n",
      "3000 0.30948088\n",
      "3200 0.30080855\n",
      "3400 0.29254702\n",
      "3600 0.28467283\n",
      "3800 0.27716395\n",
      "4000 0.2699997\n",
      "4200 0.2631606\n",
      "4400 0.25662813\n",
      "4600 0.25038514\n",
      "4800 0.24441506\n",
      "5000 0.2387027\n",
      "5200 0.23323365\n",
      "5400 0.22799426\n",
      "5600 0.22297192\n",
      "5800 0.21815474\n",
      "6000 0.21353151\n",
      "6200 0.20909168\n",
      "6400 0.20482552\n",
      "6600 0.20072371\n",
      "6800 0.1967777\n",
      "7000 0.19297926\n",
      "7200 0.18932092\n",
      "7400 0.1857955\n",
      "7600 0.18239635\n",
      "7800 0.17911713\n",
      "8000 0.17595182\n",
      "8200 0.17289506\n",
      "8400 0.16994148\n",
      "8600 0.16708624\n",
      "8800 0.16432475\n",
      "9000 0.16165252\n",
      "9200 0.15906553\n",
      "9400 0.1565599\n",
      "9600 0.15413196\n",
      "9800 0.15177831\n",
      "10000 0.14949562\n",
      "\n",
      "Hypothesis:  [[0.03074029]\n",
      " [0.15884677]\n",
      " [0.30486736]\n",
      " [0.78138196]\n",
      " [0.93957496]\n",
      " [0.9801688 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "0 0.97422194\n",
      "200 0.73380584\n",
      "400 0.68054277\n",
      "600 0.65738314\n",
      "800 0.6405061\n",
      "1000 0.62598956\n",
      "1200 0.61303777\n",
      "1400 0.6013964\n",
      "1600 0.59091467\n",
      "1800 0.58146954\n",
      "2000 0.57295144\n",
      "2200 0.56526166\n",
      "2400 0.5583115\n",
      "2600 0.5520215\n",
      "2800 0.5463204\n",
      "3000 0.54114515\n",
      "3200 0.53643954\n",
      "3400 0.5321536\n",
      "3600 0.52824324\n",
      "3800 0.5246693\n",
      "4000 0.52139693\n",
      "4200 0.51839536\n",
      "4400 0.5156375\n",
      "4600 0.51309913\n",
      "4800 0.5107587\n",
      "5000 0.50859725\n",
      "5200 0.50659776\n",
      "5400 0.5047452\n",
      "5600 0.5030261\n",
      "5800 0.5014283\n",
      "6000 0.4999411\n",
      "6200 0.498555\n",
      "6400 0.49726117\n",
      "6600 0.49605182\n",
      "6800 0.49492007\n",
      "7000 0.49385962\n",
      "7200 0.49286455\n",
      "7400 0.4919301\n",
      "7600 0.49105117\n",
      "7800 0.49022377\n",
      "8000 0.4894441\n",
      "8200 0.48870838\n",
      "8400 0.48801363\n",
      "8600 0.48735696\n",
      "8800 0.4867357\n",
      "9000 0.48614722\n",
      "9200 0.48558944\n",
      "9400 0.4850604\n",
      "9600 0.48455814\n",
      "9800 0.4840809\n",
      "10000 0.48362702\n",
      "\n",
      "Hypothesis:  [[0.3700318 ]\n",
      " [0.9154805 ]\n",
      " [0.21602735]\n",
      " [0.94834936]\n",
      " [0.08354768]\n",
      " [0.7647524 ]\n",
      " [0.94660914]\n",
      " [0.6240097 ]\n",
      " [0.24575189]\n",
      " [0.53068644]\n",
      " [0.7005222 ]\n",
      " [0.17214663]\n",
      " [0.16543275]\n",
      " [0.2199429 ]\n",
      " [0.71247756]\n",
      " [0.45385256]\n",
      " [0.73282206]\n",
      " [0.86161494]\n",
      " [0.81474245]\n",
      " [0.5555971 ]\n",
      " [0.6401038 ]\n",
      " [0.10689519]\n",
      " [0.683552  ]\n",
      " [0.7439062 ]\n",
      " [0.36421007]\n",
      " [0.9393675 ]\n",
      " [0.6559926 ]\n",
      " [0.6254447 ]\n",
      " [0.6521136 ]\n",
      " [0.40692747]\n",
      " [0.9621741 ]\n",
      " [0.8779099 ]\n",
      " [0.5816843 ]\n",
      " [0.7878031 ]\n",
      " [0.37508687]\n",
      " [0.6011569 ]\n",
      " [0.7843533 ]\n",
      " [0.39770976]\n",
      " [0.5350236 ]\n",
      " [0.3182409 ]\n",
      " [0.8445039 ]\n",
      " [0.15713443]\n",
      " [0.41641977]\n",
      " [0.03135979]\n",
      " [0.5267336 ]\n",
      " [0.9159291 ]\n",
      " [0.7015435 ]\n",
      " [0.7093997 ]\n",
      " [0.94602114]\n",
      " [0.93769354]\n",
      " [0.9446371 ]\n",
      " [0.24789676]\n",
      " [0.3219585 ]\n",
      " [0.9602661 ]\n",
      " [0.23895976]\n",
      " [0.42620692]\n",
      " [0.06667964]\n",
      " [0.7589261 ]\n",
      " [0.84101737]\n",
      " [0.5098418 ]\n",
      " [0.93055457]\n",
      " [0.7081358 ]\n",
      " [0.65229744]\n",
      " [0.85724914]\n",
      " [0.54217255]\n",
      " [0.45803586]\n",
      " [0.96486694]\n",
      " [0.76065105]\n",
      " [0.8371695 ]\n",
      " [0.70461416]\n",
      " [0.25252232]\n",
      " [0.757701  ]\n",
      " [0.9056169 ]\n",
      " [0.929743  ]\n",
      " [0.8595284 ]\n",
      " [0.7762509 ]\n",
      " [0.4248644 ]\n",
      " [0.86663973]\n",
      " [0.90810114]\n",
      " [0.9136045 ]\n",
      " [0.86388415]\n",
      " [0.8441914 ]\n",
      " [0.36394182]\n",
      " [0.80408216]\n",
      " [0.55200076]\n",
      " [0.884407  ]\n",
      " [0.5241623 ]\n",
      " [0.9030942 ]\n",
      " [0.92851454]\n",
      " [0.79115593]\n",
      " [0.8253182 ]\n",
      " [0.6437376 ]\n",
      " [0.7171907 ]\n",
      " [0.62347317]\n",
      " [0.90414244]\n",
      " [0.9790907 ]\n",
      " [0.9090113 ]\n",
      " [0.5846414 ]\n",
      " [0.16190986]\n",
      " [0.6583708 ]\n",
      " [0.63592726]\n",
      " [0.9659969 ]\n",
      " [0.67171514]\n",
      " [0.7229756 ]\n",
      " [0.88660306]\n",
      " [0.72767776]\n",
      " [0.9305425 ]\n",
      " [0.8626788 ]\n",
      " [0.568232  ]\n",
      " [0.32958367]\n",
      " [0.94322944]\n",
      " [0.8515173 ]\n",
      " [0.44822   ]\n",
      " [0.39299121]\n",
      " [0.62763005]\n",
      " [0.7908457 ]\n",
      " [0.858948  ]\n",
      " [0.93857807]\n",
      " [0.14030881]\n",
      " [0.7290221 ]\n",
      " [0.86476177]\n",
      " [0.6166415 ]\n",
      " [0.64215124]\n",
      " [0.77957445]\n",
      " [0.7059268 ]\n",
      " [0.8574369 ]\n",
      " [0.82025975]\n",
      " [0.54896337]\n",
      " [0.57049835]\n",
      " [0.31858006]\n",
      " [0.45877916]\n",
      " [0.7533743 ]\n",
      " [0.93821293]\n",
      " [0.8635385 ]\n",
      " [0.8011608 ]\n",
      " [0.8701301 ]\n",
      " [0.4200652 ]\n",
      " [0.81549966]\n",
      " [0.72602177]\n",
      " [0.7184816 ]\n",
      " [0.89642775]\n",
      " [0.6353291 ]\n",
      " [0.610517  ]\n",
      " [0.67205024]\n",
      " [0.9115738 ]\n",
      " [0.64918935]\n",
      " [0.44098884]\n",
      " [0.9371995 ]\n",
      " [0.6337202 ]\n",
      " [0.7613014 ]\n",
      " [0.23399186]\n",
      " [0.32680222]\n",
      " [0.12558192]\n",
      " [0.23778123]\n",
      " [0.9076003 ]\n",
      " [0.862341  ]\n",
      " [0.9534827 ]\n",
      " [0.12552066]\n",
      " [0.5343293 ]\n",
      " [0.80264574]\n",
      " [0.64998204]\n",
      " [0.8487989 ]\n",
      " [0.38238314]\n",
      " [0.79798627]\n",
      " [0.63381225]\n",
      " [0.5795338 ]\n",
      " [0.69325584]\n",
      " [0.8828245 ]\n",
      " [0.77005243]\n",
      " [0.6494112 ]\n",
      " [0.85252994]\n",
      " [0.85940075]\n",
      " [0.9539152 ]\n",
      " [0.2406517 ]\n",
      " [0.79474664]\n",
      " [0.2703986 ]\n",
      " [0.37937924]\n",
      " [0.3092518 ]\n",
      " [0.88998455]\n",
      " [0.68779784]\n",
      " [0.932184  ]\n",
      " [0.9017601 ]\n",
      " [0.5979744 ]\n",
      " [0.13032866]\n",
      " [0.17434558]\n",
      " [0.52043444]\n",
      " [0.74812824]\n",
      " [0.67661554]\n",
      " [0.828105  ]\n",
      " [0.6600339 ]\n",
      " [0.36263257]\n",
      " [0.18541756]\n",
      " [0.9092251 ]\n",
      " [0.41367313]\n",
      " [0.8532128 ]\n",
      " [0.9062588 ]\n",
      " [0.68145704]\n",
      " [0.6851039 ]\n",
      " [0.56336015]\n",
      " [0.55329156]\n",
      " [0.6777324 ]\n",
      " [0.95872444]\n",
      " [0.7710125 ]\n",
      " [0.8127992 ]\n",
      " [0.13762411]\n",
      " [0.3309636 ]\n",
      " [0.9089522 ]\n",
      " [0.22505347]\n",
      " [0.9303493 ]\n",
      " [0.27977678]\n",
      " [0.28565744]\n",
      " [0.520548  ]\n",
      " [0.7501163 ]\n",
      " [0.2057087 ]\n",
      " [0.75803435]\n",
      " [0.7481889 ]\n",
      " [0.7201228 ]\n",
      " [0.649068  ]\n",
      " [0.11264303]\n",
      " [0.27403602]\n",
      " [0.7192456 ]\n",
      " [0.507519  ]\n",
      " [0.9256157 ]\n",
      " [0.94974613]\n",
      " [0.70573455]\n",
      " [0.3314457 ]\n",
      " [0.01243345]\n",
      " [0.7328249 ]\n",
      " [0.31322715]\n",
      " [0.4551864 ]\n",
      " [0.95113   ]\n",
      " [0.6119204 ]\n",
      " [0.95621103]\n",
      " [0.2124781 ]\n",
      " [0.13730334]\n",
      " [0.26409647]\n",
      " [0.746002  ]\n",
      " [0.90616345]\n",
      " [0.8902337 ]\n",
      " [0.6388824 ]\n",
      " [0.5905467 ]\n",
      " [0.61427677]\n",
      " [0.1180374 ]\n",
      " [0.5649264 ]\n",
      " [0.12032915]\n",
      " [0.594708  ]\n",
      " [0.8765245 ]\n",
      " [0.65327024]\n",
      " [0.7013044 ]\n",
      " [0.96059424]\n",
      " [0.8171558 ]\n",
      " [0.7480623 ]\n",
      " [0.7085494 ]\n",
      " [0.7331733 ]\n",
      " [0.87156576]\n",
      " [0.38832107]\n",
      " [0.48885614]\n",
      " [0.4557638 ]\n",
      " [0.7863519 ]\n",
      " [0.6569421 ]\n",
      " [0.67967415]\n",
      " [0.78093886]\n",
      " [0.26716092]\n",
      " [0.3746989 ]\n",
      " [0.47442526]\n",
      " [0.62990814]\n",
      " [0.31520733]\n",
      " [0.9203258 ]\n",
      " [0.7605115 ]\n",
      " [0.9225775 ]\n",
      " [0.56285393]\n",
      " [0.7541991 ]\n",
      " [0.8239823 ]\n",
      " [0.8495199 ]\n",
      " [0.60194135]\n",
      " [0.8399879 ]\n",
      " [0.3606706 ]\n",
      " [0.64815956]\n",
      " [0.739785  ]\n",
      " [0.38866496]\n",
      " [0.7920482 ]\n",
      " [0.24882402]\n",
      " [0.5482083 ]\n",
      " [0.9495237 ]\n",
      " [0.81041384]\n",
      " [0.8621623 ]\n",
      " [0.69616514]\n",
      " [0.38970423]\n",
      " [0.6218725 ]\n",
      " [0.4183417 ]\n",
      " [0.474418  ]\n",
      " [0.651903  ]\n",
      " [0.6572863 ]\n",
      " [0.6830765 ]\n",
      " [0.54236895]\n",
      " [0.18214154]\n",
      " [0.70659906]\n",
      " [0.9211707 ]\n",
      " [0.4837174 ]\n",
      " [0.64411545]\n",
      " [0.79061466]\n",
      " [0.5516873 ]\n",
      " [0.7857443 ]\n",
      " [0.43543452]\n",
      " [0.66561157]\n",
      " [0.8926581 ]\n",
      " [0.6695023 ]\n",
      " [0.7405897 ]\n",
      " [0.8683181 ]\n",
      " [0.4409264 ]\n",
      " [0.8741252 ]\n",
      " [0.95516   ]\n",
      " [0.31215015]\n",
      " [0.814825  ]\n",
      " [0.2805749 ]\n",
      " [0.784817  ]\n",
      " [0.81692725]\n",
      " [0.72387695]\n",
      " [0.40729514]\n",
      " [0.77939034]\n",
      " [0.79634446]\n",
      " [0.72602904]\n",
      " [0.21197815]\n",
      " [0.8400166 ]\n",
      " [0.8832094 ]\n",
      " [0.43998408]\n",
      " [0.95368844]\n",
      " [0.24628602]\n",
      " [0.7256563 ]\n",
      " [0.95856625]\n",
      " [0.25511125]\n",
      " [0.38668022]\n",
      " [0.6652211 ]\n",
      " [0.32212332]\n",
      " [0.18379892]\n",
      " [0.86391807]\n",
      " [0.91269827]\n",
      " [0.8421713 ]\n",
      " [0.612567  ]\n",
      " [0.61782426]\n",
      " [0.61188555]\n",
      " [0.77833873]\n",
      " [0.81998533]\n",
      " [0.94705737]\n",
      " [0.7201809 ]\n",
      " [0.74858665]\n",
      " [0.59124976]\n",
      " [0.9264297 ]\n",
      " [0.94356465]\n",
      " [0.65994716]\n",
      " [0.28874373]\n",
      " [0.6326212 ]\n",
      " [0.3437618 ]\n",
      " [0.78724736]\n",
      " [0.18558833]\n",
      " [0.26064605]\n",
      " [0.42378923]\n",
      " [0.6634197 ]\n",
      " [0.32271567]\n",
      " [0.5886754 ]\n",
      " [0.85246617]\n",
      " [0.64628965]\n",
      " [0.8607625 ]\n",
      " [0.95665103]\n",
      " [0.7814898 ]\n",
      " [0.03902068]\n",
      " [0.3468342 ]\n",
      " [0.84605324]\n",
      " [0.8765325 ]\n",
      " [0.6374535 ]\n",
      " [0.2785793 ]\n",
      " [0.9115516 ]\n",
      " [0.8942279 ]\n",
      " [0.29613292]\n",
      " [0.5700543 ]\n",
      " [0.8259093 ]\n",
      " [0.86692053]\n",
      " [0.83554876]\n",
      " [0.8723941 ]\n",
      " [0.8982546 ]\n",
      " [0.9394259 ]\n",
      " [0.6399861 ]\n",
      " [0.610468  ]\n",
      " [0.5533838 ]\n",
      " [0.8144549 ]\n",
      " [0.8708996 ]\n",
      " [0.23334222]\n",
      " [0.7985961 ]\n",
      " [0.89199734]\n",
      " [0.279927  ]\n",
      " [0.48323703]\n",
      " [0.8379908 ]\n",
      " [0.5846791 ]\n",
      " [0.9026144 ]\n",
      " [0.30262965]\n",
      " [0.83026654]\n",
      " [0.6704346 ]\n",
      " [0.8768143 ]\n",
      " [0.38466313]\n",
      " [0.69479674]\n",
      " [0.6945852 ]\n",
      " [0.79477954]\n",
      " [0.07643868]\n",
      " [0.17290844]\n",
      " [0.6348562 ]\n",
      " [0.8155424 ]\n",
      " [0.3817097 ]\n",
      " [0.81980556]\n",
      " [0.504805  ]\n",
      " [0.38023195]\n",
      " [0.78097284]\n",
      " [0.42587543]\n",
      " [0.8984246 ]\n",
      " [0.8508325 ]\n",
      " [0.65622354]\n",
      " [0.91650057]\n",
      " [0.6692599 ]\n",
      " [0.77402496]\n",
      " [0.35418713]\n",
      " [0.32064435]\n",
      " [0.7541235 ]\n",
      " [0.46952865]\n",
      " [0.52253413]\n",
      " [0.9015023 ]\n",
      " [0.8999266 ]\n",
      " [0.912713  ]\n",
      " [0.95233107]\n",
      " [0.70150965]\n",
      " [0.8361005 ]\n",
      " [0.38574633]\n",
      " [0.3973807 ]\n",
      " [0.47271663]\n",
      " [0.9415037 ]\n",
      " [0.53401273]\n",
      " [0.18898869]\n",
      " [0.9329737 ]\n",
      " [0.84650695]\n",
      " [0.49440587]\n",
      " [0.80312306]\n",
      " [0.00960753]\n",
      " [0.9207955 ]\n",
      " [0.80055183]\n",
      " [0.782111  ]\n",
      " [0.8203358 ]\n",
      " [0.9680127 ]\n",
      " [0.60169435]\n",
      " [0.79016757]\n",
      " [0.64055496]\n",
      " [0.8574066 ]\n",
      " [0.23814832]\n",
      " [0.5444675 ]\n",
      " [0.9211141 ]\n",
      " [0.63316137]\n",
      " [0.76309925]\n",
      " [0.93186283]\n",
      " [0.83781093]\n",
      " [0.87973046]\n",
      " [0.444313  ]\n",
      " [0.8162312 ]\n",
      " [0.9568143 ]\n",
      " [0.7589753 ]\n",
      " [0.6570054 ]\n",
      " [0.3295093 ]\n",
      " [0.41978225]\n",
      " [0.57137984]\n",
      " [0.6386369 ]\n",
      " [0.53501093]\n",
      " [0.78830326]\n",
      " [0.59427124]\n",
      " [0.7511472 ]\n",
      " [0.8545512 ]\n",
      " [0.79672927]\n",
      " [0.6231361 ]\n",
      " [0.5059155 ]\n",
      " [0.61367357]\n",
      " [0.9459432 ]\n",
      " [0.8694799 ]\n",
      " [0.23167524]\n",
      " [0.4707916 ]\n",
      " [0.43242708]\n",
      " [0.08282863]\n",
      " [0.8893514 ]\n",
      " [0.14215669]\n",
      " [0.8959385 ]\n",
      " [0.86194164]\n",
      " [0.834408  ]\n",
      " [0.6286048 ]\n",
      " [0.88424885]\n",
      " [0.35417128]\n",
      " [0.7715175 ]\n",
      " [0.94070786]\n",
      " [0.38943192]\n",
      " [0.42330423]\n",
      " [0.88944346]\n",
      " [0.86383   ]\n",
      " [0.5855265 ]\n",
      " [0.80563533]\n",
      " [0.79816794]\n",
      " [0.7988252 ]\n",
      " [0.35251015]\n",
      " [0.74067354]\n",
      " [0.8714471 ]\n",
      " [0.5967558 ]\n",
      " [0.7951093 ]\n",
      " [0.76162237]\n",
      " [0.8073532 ]\n",
      " [0.8396836 ]\n",
      " [0.94706494]\n",
      " [0.67303115]\n",
      " [0.4091976 ]\n",
      " [0.78243226]\n",
      " [0.7520775 ]\n",
      " [0.97216964]\n",
      " [0.79477125]\n",
      " [0.7043549 ]\n",
      " [0.370243  ]\n",
      " [0.7241599 ]\n",
      " [0.91122055]\n",
      " [0.9551282 ]\n",
      " [0.92075676]\n",
      " [0.7272074 ]\n",
      " [0.62702626]\n",
      " [0.8090792 ]\n",
      " [0.4219896 ]\n",
      " [0.768348  ]\n",
      " [0.7709796 ]\n",
      " [0.8537084 ]\n",
      " [0.6119234 ]\n",
      " [0.7090311 ]\n",
      " [0.8534461 ]\n",
      " [0.48009148]\n",
      " [0.476007  ]\n",
      " [0.63377106]\n",
      " [0.73435646]\n",
      " [0.5865755 ]\n",
      " [0.92218083]\n",
      " [0.9280279 ]\n",
      " [0.24493484]\n",
      " [0.11743274]\n",
      " [0.8053016 ]\n",
      " [0.5571552 ]\n",
      " [0.22469106]\n",
      " [0.8327048 ]\n",
      " [0.9014041 ]\n",
      " [0.6787964 ]\n",
      " [0.94124955]\n",
      " [0.91504407]\n",
      " [0.80182153]\n",
      " [0.8258106 ]\n",
      " [0.6871733 ]\n",
      " [0.57794136]\n",
      " [0.76354223]\n",
      " [0.61091614]\n",
      " [0.14149426]\n",
      " [0.9059014 ]\n",
      " [0.89553386]\n",
      " [0.681404  ]\n",
      " [0.91510576]\n",
      " [0.8662421 ]\n",
      " [0.9031191 ]\n",
      " [0.6343755 ]\n",
      " [0.7443474 ]\n",
      " [0.868535  ]\n",
      " [0.714225  ]\n",
      " [0.87389636]\n",
      " [0.9243368 ]\n",
      " [0.5383593 ]\n",
      " [0.8247157 ]\n",
      " [0.82216066]\n",
      " [0.45614845]\n",
      " [0.5516042 ]\n",
      " [0.07508863]\n",
      " [0.25541744]\n",
      " [0.84423894]\n",
      " [0.66269827]\n",
      " [0.6723399 ]\n",
      " [0.48981878]\n",
      " [0.9319752 ]\n",
      " [0.46693403]\n",
      " [0.8121207 ]\n",
      " [0.23049167]\n",
      " [0.8926242 ]\n",
      " [0.27195424]\n",
      " [0.79920727]\n",
      " [0.5422217 ]\n",
      " [0.7750569 ]\n",
      " [0.5572491 ]\n",
      " [0.27887404]\n",
      " [0.772218  ]\n",
      " [0.9362968 ]\n",
      " [0.3985843 ]\n",
      " [0.9279789 ]\n",
      " [0.86264396]\n",
      " [0.85393304]\n",
      " [0.8187632 ]\n",
      " [0.42240217]\n",
      " [0.34413046]\n",
      " [0.64388883]\n",
      " [0.13661328]\n",
      " [0.95413154]\n",
      " [0.39051205]\n",
      " [0.93976825]\n",
      " [0.8957867 ]\n",
      " [0.43772212]\n",
      " [0.18767704]\n",
      " [0.63602334]\n",
      " [0.46848196]\n",
      " [0.8321993 ]\n",
      " [0.71974903]\n",
      " [0.98327386]\n",
      " [0.43412197]\n",
      " [0.6611377 ]\n",
      " [0.79943466]\n",
      " [0.65891474]\n",
      " [0.04487875]\n",
      " [0.7660401 ]\n",
      " [0.7920545 ]\n",
      " [0.8538984 ]\n",
      " [0.6441347 ]\n",
      " [0.45210445]\n",
      " [0.6059911 ]\n",
      " [0.90392   ]\n",
      " [0.58050025]\n",
      " [0.8184375 ]\n",
      " [0.80364263]\n",
      " [0.88844115]\n",
      " [0.8134402 ]\n",
      " [0.56198764]\n",
      " [0.786167  ]\n",
      " [0.9003914 ]\n",
      " [0.67731243]\n",
      " [0.96727574]\n",
      " [0.809111  ]\n",
      " [0.615409  ]\n",
      " [0.50250506]\n",
      " [0.79200083]\n",
      " [0.8376326 ]\n",
      " [0.4987446 ]\n",
      " [0.6958789 ]\n",
      " [0.28857082]\n",
      " [0.63699603]\n",
      " [0.8366307 ]\n",
      " [0.9552349 ]\n",
      " [0.84366626]\n",
      " [0.77009255]\n",
      " [0.7299969 ]\n",
      " [0.90502757]\n",
      " [0.4980013 ]\n",
      " [0.9446072 ]\n",
      " [0.4642227 ]\n",
      " [0.7902366 ]\n",
      " [0.34313145]\n",
      " [0.05604664]\n",
      " [0.33493057]\n",
      " [0.36218202]\n",
      " [0.6980837 ]\n",
      " [0.8445837 ]\n",
      " [0.5875374 ]\n",
      " [0.76824254]\n",
      " [0.8124061 ]\n",
      " [0.59036416]\n",
      " [0.39017186]\n",
      " [0.8804146 ]\n",
      " [0.9080004 ]\n",
      " [0.34558284]\n",
      " [0.60365367]\n",
      " [0.20400006]\n",
      " [0.41466662]\n",
      " [0.7431205 ]\n",
      " [0.7132845 ]\n",
      " [0.895306  ]\n",
      " [0.9793977 ]\n",
      " [0.20553096]\n",
      " [0.75263506]\n",
      " [0.5752076 ]\n",
      " [0.3756154 ]\n",
      " [0.72092   ]\n",
      " [0.73230755]\n",
      " [0.89490426]\n",
      " [0.7099159 ]\n",
      " [0.49394774]\n",
      " [0.58987445]\n",
      " [0.17812283]\n",
      " [0.65061325]\n",
      " [0.55043185]\n",
      " [0.9054695 ]\n",
      " [0.598595  ]\n",
      " [0.63834715]\n",
      " [0.80110824]\n",
      " [0.73684454]\n",
      " [0.37900406]\n",
      " [0.75590736]\n",
      " [0.61501664]\n",
      " [0.2606364 ]\n",
      " [0.580967  ]\n",
      " [0.91178626]\n",
      " [0.8376616 ]\n",
      " [0.6030847 ]\n",
      " [0.8069207 ]\n",
      " [0.32273334]\n",
      " [0.8301623 ]\n",
      " [0.62737674]\n",
      " [0.78778106]\n",
      " [0.40930733]\n",
      " [0.6811735 ]\n",
      " [0.8340646 ]\n",
      " [0.15815164]\n",
      " [0.27006996]\n",
      " [0.78574777]\n",
      " [0.81408614]\n",
      " [0.76922286]\n",
      " [0.90127355]\n",
      " [0.7859849 ]\n",
      " [0.7243077 ]\n",
      " [0.7613378 ]\n",
      " [0.74655616]\n",
      " [0.68669134]\n",
      " [0.7927316 ]\n",
      " [0.50835574]\n",
      " [0.4611718 ]\n",
      " [0.8820466 ]\n",
      " [0.82784206]\n",
      " [0.66061914]\n",
      " [0.29658586]\n",
      " [0.88612473]\n",
      " [0.76964337]\n",
      " [0.82751465]\n",
      " [0.69222766]\n",
      " [0.86061233]\n",
      " [0.86218196]\n",
      " [0.73982066]\n",
      " [0.402918  ]\n",
      " [0.9043559 ]\n",
      " [0.92673653]\n",
      " [0.2923935 ]\n",
      " [0.14486505]\n",
      " [0.75177014]\n",
      " [0.38205963]\n",
      " [0.7350758 ]\n",
      " [0.36591637]\n",
      " [0.4724523 ]\n",
      " [0.3321533 ]\n",
      " [0.7976325 ]\n",
      " [0.873673  ]\n",
      " [0.1587571 ]\n",
      " [0.37841156]\n",
      " [0.54074955]\n",
      " [0.49625945]\n",
      " [0.51185215]\n",
      " [0.77610594]\n",
      " [0.16323733]\n",
      " [0.91351545]\n",
      " [0.1940887 ]\n",
      " [0.85807306]\n",
      " [0.7474166 ]\n",
      " [0.7190663 ]\n",
      " [0.8625043 ]\n",
      " [0.68273556]\n",
      " [0.886802  ]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.76679844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1444304\n",
      "200 0.5701725\n",
      "400 0.46664643\n",
      "600 0.3719219\n",
      "800 0.28220817\n",
      "1000 0.23855816\n",
      "1200 0.21593893\n",
      "1400 0.1971462\n",
      "1600 0.18127106\n",
      "1800 0.16768387\n",
      "2000 0.15592827\n",
      "--------------\n",
      "[[7.4781636e-03 9.9251574e-01 6.1860369e-06]] [1]\n",
      "--------------\n",
      "[[0.8336169 0.1587082 0.0076748]] [0]\n",
      "--------------\n",
      "[[9.386088e-09 2.992087e-04 9.997008e-01]] [2]\n",
      "--------------\n",
      "[[7.4781636e-03 9.9251574e-01 6.1860374e-06]\n",
      " [8.3361691e-01 1.5870818e-01 7.6748026e-03]\n",
      " [9.3860884e-09 2.9920871e-04 9.9970078e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    all = sess.run(hypothesis, feed_dict={\n",
    "                   X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "one_hot Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
      "Step:     0\tLoss: 3.509\tAcc: 25.74%\n",
      "Step:   100\tLoss: 0.685\tAcc: 83.17%\n",
      "Step:   200\tLoss: 0.454\tAcc: 88.12%\n",
      "Step:   300\tLoss: 0.342\tAcc: 91.09%\n",
      "Step:   400\tLoss: 0.274\tAcc: 93.07%\n",
      "Step:   500\tLoss: 0.226\tAcc: 94.06%\n",
      "Step:   600\tLoss: 0.192\tAcc: 94.06%\n",
      "Step:   700\tLoss: 0.165\tAcc: 95.05%\n",
      "Step:   800\tLoss: 0.144\tAcc: 96.04%\n",
      "Step:   900\tLoss: 0.128\tAcc: 98.02%\n",
      "Step:  1000\tLoss: 0.115\tAcc: 98.02%\n",
      "Step:  1100\tLoss: 0.104\tAcc: 99.01%\n",
      "Step:  1200\tLoss: 0.095\tAcc: 100.00%\n",
      "Step:  1300\tLoss: 0.087\tAcc: 100.00%\n",
      "Step:  1400\tLoss: 0.081\tAcc: 100.00%\n",
      "Step:  1500\tLoss: 0.075\tAcc: 100.00%\n",
      "Step:  1600\tLoss: 0.071\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.066\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.062\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.059\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data/data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                 labels=tf.stop_gradient([Y_one_hot]))\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: x_data, Y: y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
